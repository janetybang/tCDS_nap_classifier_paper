---
title             : "An automated classifier for child-directed speech from LENA recordings"
shorttitle        : "Child-directed speech classifier"

author: 
  - name          : "Janet Bang"
    affiliation   : "1,2"
    corresponding : yes    # Define only one corresponding author
    address       : "Postal address"
    email         : "my@email.com"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing
  - name          : "George Kachergis"
    affiliation   : "1"
  - name          : "Adriana Weisleder"
    affiliation   : "1"
    role:
      - Writing - Review & Editing
  - name          : "Virginia Marchman"
    affiliation   : "1"
    role:
      - Conceptualization
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Stanford University"
  - id            : "2"
    institution   : "San Jose State University"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  Recent advances in recording technology have provided unique opportunities to observe children’s everyday speech environments using daylong audio recordings. 
  A growing number of studies have proposed that speech may support learning differently when speech is directed to  a child (target child directed speech, tCDS) than when that speech is directed to others (ODS) (Shneidman & Goldin-Meadow, 2012; Weisleder & Fernald, 2013). 
  To identify  periods of tCDS, researchers typically rely on the time-consuming and laborious work of human listeners who must consider numerous features to distinguish periods of tCDS from ODS. 
  Human listeners are also used to identify periods when children are sleeping or awake. 
  In this paper, we detail our efforts to automate this process. 
  We analyzed over 1,000 daylong recording hours from 153 English- and Spanish-speaking families in the U.S. with 17- to 28-month-old children that had been previously coded for periods of sleep, tCDS, and ODS. 
  We first conducted analyses to explore patterns of features that characterized periods of sleep, tCDS, or ODS periods. 
  Then, we evaluated two automated classifiers to identify periods of (1) sleep, and (2) tCDS versus ODS. 
  Classifiers were trained using automated measures generated from LENA, including speech frequency (AWC, CTC, CVC) and duration measures (meaningful speech, distant speech, TV, noise, silence). 
  Results revealed high sensitivity and specificity in classifying periods of sleep, and moderate sensitivity and specificity in classifying periods of tCDS and ODS. 
  In addition, model-derived predictions from our tCDS/ODS classifier yielded similar patterns of correlations as previously-published findings, with variation in tCDS, but not ODS, positively linked to children’s later vocabularies (Weisleder & Fernald, 2013). 
  This work offers promising tools for streamlining work with daylong recordings, thereby facilitating research that aims to better understand how children learn from their everyday speech environments.
  
keywords          : "Individual differences, Language input, Parent-child interaction"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library(papaja)
library(tidyverse)
library(here)
library(NbClust)
library(e1071)
library(MASS)
#library(relaimpo)
library(DiagrammeR)
library(data.table)
library(rpart)
library(rpart.plot)
library(caret)
library(xgboost)
library(pROC)
#library(xgboostExplainer)
library(randomForest)
library(ggpubr)

r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, warning = F)
```

# Introduction

Speech environments vary across children in numerous ways. 
The ability to document variation in children’s naturally-occurring speech  environments has been greatly assisted by technology that can capture, store, and process large amounts of audio data (e.g., an entire day). 
One notable example is the LENA digital language processor and software system (Gilkerson et al., 2017). 
The recorder is worn inside a child’s front shirt pocket and each recording stores up to 16 hours of the audio environment around the child. 
The LENA software automatically segments and classifies the audio recording into relevant categories (e.g., silence, speech, television) and estimates the number of adult words spoken near and clear to the child (Adult Word Count, AWC), as well as the number of child vocalizations (Child Vocalization Count, CVC) and conversational turns (Conversational Turn Count, CTC). 
To date, studies using LENA recordings have been conducted in numerous languages, as well as across a variety of sociocultural settings (for a systematic review of validation studies see Cristia et al., 2020).

While the automated counts provided by LENA are very useful, these measures gloss over many important features of audio environments. 
For example, it is not possible to know from the LENA automated counts if silence represents periods when the child is sleeping, or if the child is awake but no speech is addressed to them. 
Automatic estimates of AWC include all adult speech that is spoken near to the child, but does not distinguish between speech that is directed to the child versus speech that is addressed to other adults or children. 
The fact that automated estimates of AWC do not specifically reflect the child-directed nature of speech has received particular attention. 
A growing body of work has proposed that child-directed speech, more so than other-directed speech, supports lexical development (Ramírez-Esparza et al., 2014; Shneidman & Goldin-Meadow, 2012; Weisleder & Fernald, 2013) and that a child-directed register is characterized by certain acoustic and prosodic properties that may be particularly conducive to learning (Fernald et al., 1989; Singh et al., 2009; Soderstrom, 2007; Stärk et al., 2022, 2022). 
There is therefore growing interest in identifying periods of child-directed speech within daylong recordings. 
This paper presents an automated, open-source classifier with the potential to facilitate this process.

## Child-directed versus other-directed speech

The construct of child-directed speech is central to theories that aim to explain how children learn language from social interactions (Csibra & Gergely, 2009; Tomasello, 1995). 
However, some communities vary widely in how much speech is directed to children (Casillas et al., 2019; Ochs & Schieffelin, 1984; Shneidman & Goldin-Meadow, 2012) and how much speech is spoken around the child, but not directed to them. 
Despite this variability, cross-cultural work finds that key language milestones (e.g., onset of first words and multi-word utterances) emerge around the same age in a variety of communities (Casillas et al., 2019; Crago et al., 1997). 
Such findings raise questions regarding how speech in children’s environments that may be overheard (i.e., not explicitly directed at them) can also support their language acquisition. 

Indeed, lab-based experimental studies have demonstrated that children can learn new words from speech that is not explicitly directed to them. 
For example, Akhtar and colleagues (2001 (2001) found that 1- to 2-year-old children were able to learn novel nouns and verbs when observing two adults play a game. 
Other studies varied the degree of joint attention between speaker and learner, such as having speakers turn their backs to infants during a word learning episode, replicating the finding that children can learn new words even in contexts when speech is not directed to them (Gampe et al., 2012). 
In contrast, research examining speech in natural environments reports that child-directed speech, more so than other-directed speech, is associated with children’s vocabulary development. 
For example, using LENA recordings with 29 Spanish-speaking families in the U.S., Weisleder and Fernald (2013) recorded speech environments and then hand-coded periods of child-directed speech, when speech was directed to the target child in one-on-one interactions or with others, or overheard speech, when speech was directed to other adults or children other than the target child. 
Using AWCs from LENA, they found that the number of adult words in periods with target-child-directed speech at 19 months was related to children’s vocabulary size at 25 months, while the number of adult words in periods with other-directed speech was not. 
Similar findings were observed in Shneidman and Goldin-Meadow (2012), where the amount of child-directed, but not overheard, speech was associated with vocabulary skills in Yucatec-Mayan-speaking families in subsistence farming communities in Mexico. 
Vocabulary was measured using receptive and expressive vocabulary tasks that were adapted for Mayan speakers. 
Collectively, these studies reveal mixed findings about the differential roles of child-directed and overheard speech in young children’s language learning. 

Another observation about child-directed speech is that, when caregivers engage with young children, they may change their speech register, producing a type of speech colloquially referred to as “baby talk” or “parentese”.
Numerous acoustic, prosodic, phonological, lexical, grammatical, and pragmatic features have been noted to differentiate child-directed versus adult-directed registers (Hilton et al., 2020; Soderstrom, 2007). 
Moreover, speech that is characterized by some child-directed speech features has been suggested to support children’s speech and language acquisition (Byers-Heinlein et al., 2021; Fernald et al., 1989; Singh et al., 2009; Snow, 1977). 
For example, a recent multi-continent collaboration demonstrated that speech characterized by the acoustic and phonological features of North American English CDS register attracts the attention of both mono- and bilingually-exposed infants more so than speech spoken in an adult-directed register. 
These results were interpreted to suggest  that  acoustic features associated with the child-directed speech register may be more effective at attracting infants’ attention and thereby, can better support learning (Byers-Heinlein et al., 2021). 
However, there is continued debate about the role of child- and adult-directed speech in children’s language learning across linguistic and cultural contexts (Solomon, 2011; Cox et al., 2022). 

## Identifying periods of sleep, target- and other-directed speech in daylong recordings

Daylong recordings, such as those enabled by the LENA recording technology, provide an extraordinary opportunity to examine variation in the features of target-child-directed and other-directed speech in children’s natural environments. To examine this variation, it is first necessary to identify periods during the recording which are characterized by each type of speech. 

In studies to date, human listeners are trained to identify periods of sleep, child-directed, and other-directed speech by attending to numerous cues that are available on the audio recording. 
However, less is known about which of the multiple available features most reliably characterize these different periods. 
Moreover, while fruitful, these efforts are highly labor and time intensive. 
Though there are emerging tools to support the rigor and efficiency of this type of manual coding (Cychosz et al., 2021; Mendoza & Fausey, 2021), efforts to create automated tools are also in critical need. 
Additionally, in some cases, ethical considerations prevent researchers from listening to the recordings (Cychosz et al., 2020). 
Thus, tools that enable classification of periods of child-directed and other-directed speech from features that are automatically extracted from the recordings could expand the range of cases in which such features can be examined.

LENA provides automated measures that characterize the child’s audio environment (Xu et al., 2009), including speech frequency measures of adult word counts (AWC), child vocalizations, and conversational turns between adults and the target child, and several time-based estimates of noise, silence, distant speech (i.e., duration of speech far from the target child or overlapping), meaningful speech (i.e., duration of speech that is near and clear to the child), and electronic media (e.g., TV). Speech frequency measures of AWC, CTC, and CVC are all calculated from speech that LENA classifies as “meaningful,” i.e., speech that is near and clear to the child. 
All vocalization counts include speech-like vocalizations, but exclude respiratory (e.g., breathing) and digestive sounds (e.g., burping). 
Moreover, each vocalization is counted toward speaker types (e.g., adult female, adult male, target child, non-target child) whenever speech is separated by a 300 ms break. 
One vocalization may consist of single or multiple words. Conversational turns are defined as a sequence of alternating child-to-adult or adult-to-child vocalizations that are separated by no more than 5 seconds. 
To derive estimates of AWC, CVC, and CTC during day long recordings, it is often standard practice to normalize across recordings of different lengths to obtain a rate over some unit time, e.g., AWC/hour. 
In this computation, it may also be appropriate to remove segments of the recording when the child is not available to listen to that speech, for example, when the child is sleeping. 

Figure 1 illustrates examples from three children’s daylong recordings (Weisleder & Fernald, 2013), illustrating the automated AWC estimates per 5-min audio segments across the day. 
For each segment, human listeners judged whether the child was sleeping and whether the adult speech was primarily tCDS or ODS. 
Notably, coding revealed that segments with a range of both high and low AWC values could be identified as tCDS and ODS.

(insert Figure 1 about here)

Typically, these automated speech measures are evaluated independently, ranking individual families as having higher vs. lower mean AWCs or CTCs, or ranking children as having higher vs. lower CVCs. 
However, it is also possible to use these measures in conjunction to distinguish more subtle differences among periods of time during the daylong recordings. 
For example, for a given 5-minute segment, estimates of AWC may be more likely to be tCDS when accompanied by relatively high values of CTC or CVC. 
Or, a child is more likely to be sleeping when low values of AWC are also accompanied by low values of CVC or CTC. 
Finally, segments with relatively more minutes of distant speech may be more likely to be ODS than segments with more minutes of meaningful speech. By combining these measures in various ways, we can gain insights into which features characterize periods of tCDS and ODS in a child’s environment and how best to identify them automatically. 

# Current study

In this paper, we present new tools that facilitate the identification of target-child-directed vs. other-directed speech in day-long LENA recordings, as well as periods when children are awake versus sleeping. 
We first examined ways to combine the frequency count measures from LENA, i.e., AWC, CTC and CVC, derived from recordings of 29 Spanish-speaking families (Weisleder & Fernald, 2013). 
We assessed the degree to which variation in these measures was associated with whether a particular 5-minute segment was classified as tCDS or ODS by human coders. 
Next, we applied more sophisticated machine-learning classifiers that combined multiple frequency- and time-based measures from LENA to identify periods of sleep, tCDS, and ODS. 
We first used cluster analyses to examine how multiple LENA features hang together to predict the judgements of human listeners. 
We then trained a sleep classifier and a tCDS/ODS classifier, comparing the results of both classifiers to human coders in a large sample of 153 English- and Spanish-speaking families. 
Finally, we examined if AWC values based on model-predicted segments of tCDS versus ODS replicated previously published links with children’s later language outcomes (Weisleder & Fernald, 2013).

In this work, we focus on the directed nature of speech to target children (i.e., those who are wearing the recorder) within the context of proximal available speech to the child. 
For English- and Spanish-speaking male and female caregivers living in the United States, speech directed to the target child may include features characteristic of a CDS register (Ferjan Ramirez et al., 2022), but this was not a requisite feature for our goals. 
We instead aim to identify speech directed to the child during one-on-one interactions or in multi-party interactions inclusive of the target child which may or may not conform to a CDS register. 
In doing so, we align our research questions with theoretical proposals that children learn language through meaningful interactions, and particularly through language input that is contingent on or relevant to the child’s vocalizations, actions, and/or attentional focus (Goldstein & Schwade, 2008; McGillion et al., 2013; Tamis-LeMonda et al., 2014; Tomasello, 1995; Yurovsky, 2018), which is more likely to take place in interactions that include the target child as a participant.


# Method

We analyzed daylong LENA recordings across five samples from a total of 153 families with 17- to 28-month-old children. 
For all samples, human listeners had coded 5-min or 10-min audio segments for periods of sleep, tCDS, or ODS following similar protocols. 
We first conducted a logistic regression to assess relations among AWC, CTC, and CVC in a preliminary sample of n = 29 families (Weisleder & Fernald, 2013). 
Next, including data from all 153 families across the five samples, we then conducted a cluster analysis and trained automated classifiers using the hand-coded segments and LENA-derived measures, including features of speech (AWC, CTC, CVC) and features based on time (meaningful speech, distant speech, TV, noise, and silence). 

(G copied from Google doc up to here)

```{r, load-data, echo=F}
load("data/combined_data_5min.Rdata")

d_allfeat <- d

# Remove fat_ed and mom_ed
#d$time = NULL
#d$fat_ed = NULL
#d$mom_ed = NULL

Nclass = table(d$cds_ohs) 
#    0     1 split 
# 3519  5272  1017 
propCDS = Nclass["1"] / sum(Nclass)
Nsubj = length(unique(d$id)) # 153 participants

```



## Participants

There are `r nrow(d)` datapoints from `r Nsubj` participants (29 Spanish-speaking families from Weisleder & Fernald, 45 English-speaking and 45 Spanish-speaking families from CONTX, 27 full-day recordings of English-speaking 18-month-olds from SOT Stanford, and 29 Spanish-speaking 18-month-olds from SOT Outreach.
The table below shows per dataset the number of segments of each type and participants. 
Note that the total number of participants is 175 because 22 participants from CONTX were also included in SOT Outreach (at a different age?).

Individual participants contributed a median of `r median(table(d$id))` segments, but the distribution is somewhat skewed: `r length(which(table(d$id)<50))` contributed fewer than 50 segments. 
The many participants with few segments are largely from the CONTX dataset, in which participants' segments were sorted in order of decreasing AWC, and segments were only annotated until 6 10-minute segments of primarily child-directed speech (CDS) were found.

```{r, data-per-subject, caption="Number of 5-minute segments and participants (N) per dataset."}
# width=3.5, height=3.0,
#hist(sort(table(d$id)), ylab="# of Participants", xlab="# of Segments", main="")
dataset_tab <- table(d$Dataset, d$cds_ohs)
dataset_tab <- cbind(dataset_tab, rowSums(dataset_tab))

dataset_subj <- d %>% distinct(Dataset,id) %>% 
  group_by(Dataset) %>% summarise(N=n())

dataset_tab <- cbind(dataset_tab, dataset_subj$N)
colnames(dataset_tab) = c("ODS","CDS","Nap","Split","Total","N")

dataset_tab <- rbind(dataset_tab, colSums(dataset_tab))
rownames(dataset_tab)[5] = "Total"
# Note: length(unique(d$id)) shows 153 subject IDs, but per dataset we have 175 -- 22 repeat IDs, actually the same kids, or recycled IDs?
knitr::kable(dataset_tab)
```

Note: CONTX actually has a much lower ratio of CDS:ODS than the other datasets, but naively we would expect it should have more CDS since the densest hour is annotated first. 
A few possible explanations: 1) there is actually a lot of ODS in the highest-AWC segments, 2) the RAs threshold for CDS was somewhat higher in CONTX, or 3) CDS comes in short bursts (<3 mins?), meaning that there are a lot 10-min chunks that have 2-3 mins of CDS but are primarily ODS. 
When we do reliability on a chunk of CONTX (say, 100 10-min segments, as 5-min segments), if we see a disproportionate number of the 10-min segments split into 1 CDS and 1 ODS, then explanation 3 seems likely.

## Material

## Procedure

## Data analysis

<!-- We used `r cite_r("r-references.bib")` for all our analyses. -->

<!-- All of the segments were first normalized -->
The table below simply shows the average LENA values for each 5-minute segment, split by the class. 
These averages show largely distinct profiles for the 1,879 nap segments, with low mean values of CVC (1.28), CTC (0.29), AWC (13.11), and meaningful (0.12), and high values of silence (3.49) and TV (0.66) as compared to all other segment types.
However, the differences between OHS, CDS, and split segments are less clear: although CDS has the highest mean values of CVC (20.94) and CTC (6.61), it also has higher mean silence and noise than the other types, and intermediate values of other variables: OHS segments have higher AWC (153.81) and split segments have nearly as high CVC (15.42) and CTC (4.80).
At a glance, it seems as though the raw LENA values may not offer us a simple way to separate CDS, OHS, and split segments.
Before constructing and training a more sophisticated classifier to identify child-directed speech, we first look in greater detail at how the speech segments cluster in this dataset.

```{r, echo=F}
overall_stats = d %>% 
  mutate(Type = case_when(
    cds_ohs=="0" ~ "Overheard Speech",
    cds_ohs=="1" ~ "Child-directed Speech",
    cds_ohs=='nap' ~ "Napping",
    cds_ohs=='split' ~ "Split CDS/OHS",
    TRUE ~ NA_character_,
  )) %>%
  group_by(Type) %>% 
  summarise(CVC=mean(CVC), 
            CTC=mean(CTC), 
            AWC=mean(AWC), 
            distant=mean(distant_min), 
            noise=mean(noise_min), 
            meaningful=mean(meaningful_min),
            tv=mean(tv_min),
            silence=mean(silence_min), N=n()) %>% 
  arrange(CVC)
#knitr::kable(overall_stats, digits=2, caption= "Means for LENA variables by category.")
apa_table(overall_stats, digits = 2, caption= "Means for LENA variables by category.")
```


## Clustering Speech Segments

We first cluster the `r Nclass['1']` segments that are labeled as child-directed speech, applying the k-means clustering algorithm for $k=\{2,..,15\}$  clusters.
We separately do the same for all `r nrow(d)-Nclass['1']` segments that are not pure CDS (naps, overheard, and split segments).
Based on the weighted sum of squares vs. $k$ plots below, we choose $k=5$ for both the child-directed speech and for the non-CDS data.

```{r, cluster-cds, echo=F, caption="Weighted sum of squares vs. number of clusters."}
source("two-class.R")
dat <- add_features(d, with_demo=F, prop_meaningful=F, per_child_norms=F)
dat$cds_ohs = d$cds_ohs

all_dat_sc = scale(dat %>% dplyr::select(-cds_ohs)) 

# clustering/plotting done in 02-clustering.Rmd
```



We cluster all 12,936 segments without regard to type, and below show the means for the $k=7$ clusters, along with the proportion of each type of segment in the cluster.
Clusters 4 and 2 capture mostly naps (63% and 58%) with low AWC, CTC, and CVC, but both clusters also include a fair number of CDS segments (22% and 26%).
Clusters 1 and 6 are predominantly CDS (59% and 73%) and cover 36.5% of the dataset, with somewhat high mean AWC values (21.7 and 54.3), CTC (1.2 and 3.8), and CVC (4.8 and 9.5), but these clusters also contain many OHS segments (39% and 27%).
Clusters 5 and 3 are mostly comprised of OHS segments (65% and 51%), the former with high values AWC (75.9 vs. 13.1), and both with low values of CTC (1.4 and 0.4) and CVC (2.6 and 1.8).
However, clusters 5 and 3 also contain a large proportion of CDS segments (34% and 45%). 
Finally, cluster 7, which looks much like the nap clusters (4 and 2) except with a higher level of TV (0.7), contains equal amounts of CDS (28%) and nap segment (27%), with somewhat more OHS segments (45%).
Overall, clustering the segments according to their raw LENA values has shown 1) that multiple LENA features capture meaningful variation between the clusters, as some correspond mostly to naps, CDS, or OHS, and yet 2) that the clusters have significant overlap in CDS and OHS, and to a lesser extent naps.
Given the large apparent differences between nap segments and all other segments, we first attempt to build a classifier that automatically distinguishes nap from non-nap segments using only raw LENA features. 
An effective nap classifier would significantly ease the burden of manual coding day-long LENA recordings.
<!-- , and will help explain the use of more complex machine learning classifiers for distinguishing CDS and OHS. --> 

```{r, all-clusters, echo=F}
load("data/all_clusters_raw_lena_5mins.Rdata") # k-means 7 clusters

all_dat_cl <- all_dat_cl %>% 
  mutate(nap = ifelse(cds_ohs=="nap", 1, 0),
         cds = ifelse(cds_ohs==1, 1, 0),
         ohs = ifelse(cds_ohs==0 | cds_ohs=="split", 1, 0),
         #split = ifelse(cds_ohs=="split", 1, 0) # lump into OHS or keep separate?
         #cluster = kfit$cluster
        ) %>% dplyr::select(-cds_ohs)


all_tab <- all_dat_cl %>% group_by(cluster) %>%
  #relocate(cluster, .before=AWC) %>%
  summarise(cluster = median(cluster), 
            N = n(),
            sleep = mean(nap),
            tCDS = mean(cds),
            ODS = mean(ohs),
            AWC = mean(AWC),
            CTC = mean(CTC),
            CVC = mean(CVC),
            noise = mean(noise),
            silence = mean(silence),
            distant = mean(distant),
            TV = mean(tv),
            meaningful = mean(meaningful)
            ) %>%
  mutate(cluster = as.factor(cluster))


require(kableExtra)
#knitr::kable(all_tab %>% arrange(cds, ohs), digits=2, caption= "Means of LENA variables by cluster.")
#apa_table(all_tab %>% arrange(cds, ohs), digits=2, caption= "Means of LENA variables by cluster.") 

all_tab_reorder = all_tab[c(4,5, 6,1, 7,2,3),]

#all_tab_reorder %>% arrange(desc(CDS), desc(ODS)) 
all_tab_reorder %>%
  kbl(caption = "Means of LENA variables by cluster.", digits=2) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  column_spec(3, bold=c(T,T,F,F,F,F,F)) %>%
  column_spec(4, bold=c(F,F,T,T,F,F,F)) %>%
  column_spec(5, bold=c(F,F,F,F,T,T,T)) %>%
  column_spec(1, italic=T) 
  #row_spec(1:2, bold = T, color = "white", background = "#D7261E") %>%
  #row_spec(3:4, bold = T, color = "white", background = "green") %>%
  #row_spec(5:6, bold = T, color = "white", background = "blue")
```


# Identifying Sleep 

We now build a simple decision tree classifier to distinguish sleep segments from waking segments, mirroring the first step that researchers often have to perform manually to clean the dataset.
We train the model using 5-fold cross-validation on 90% of the 12,931 segments (`r Nclass["nap"]` sleep segments, `r nrow(d)-Nclass["nap"]` awake segments).

```{r nap-classifier, fig.width=6, fig.height=3.2, caption="This simple decision tree (left) distinguishes naps from waking segments from normalized (per-minute) LENA values, splitting first on meaningful"}
nap_dtree = readRDS(here("models/sleep_classifier.Rds"))
nap_auc = round(nap_dtree$tree.roc$auc, 3)

knitr::include_graphics("figs/sleep_classifier.pdf")
```

The decision tree achieves an AUC of `r nap_auc` on the held-out test set of 1294 segments. 
Shown above, the decision tree splits first on the amount of "meaningful" speech per minute, and then on silence. 
If meaningful speech per minute is >0.4s and silence is less than 54s per minute (i.e. 0.9), then segments are highly likely to non-naps (96.5%, 9701 of 10050 segments).
If meaningful speech per minute is very low (<0.0063, i.e. 0.4s) and silence per minute is high (>.64, i.e. 38.4s), then these segments are highly likely to be naps (96.6%, i.e. 1017 of 1053 segments). 
If meaningful speech is very low but silence per minute is also low, then when child vocalizations (CVC) are low (<6s/min) and the amount of distant speech is low (<15s/min), then segments are also somewhat more likely to be naps (71.7%, 231 of 322 segments), although hand-coding this much smaller number of segments may be desirable to achieve higher accuracy.

Having found that raw LENA features can be effectively used to separate sleep from awake, we now turn to the more challenging task of building a classifier to automatically distinguish CDS from non-CDS segments.
While a simple decision tree performs very well for the simple case of naps, it is unlikely to work well for the overlapping clusters of child-directed and overheard speech. 
Thus, we will instead use a more sophisticated machine learning model: XGBoost (eXtreme Gradient-Boosted trees), a state-of-the-art classifier consisting of a cascade of decision trees that are successively trained on the datapoints that were misclassified by the earlier decision trees.

# Classifying Child-directed vs. Overheard speech

We trained a classifier on raw LENA features to distinguish child-directed speech segments from all other segments (overheard speech, and split segments).
First, we removed the `r Nclass["nap"]` segments during which children were napping (assuming they would be cleaned by hand or removed by the nap classifier).
We then reclassified the `r Nclass["split"]` 'split' segments which raters judged to be 50% overheard speech (OHS) and 50% child-directed speech (CDS) as ODS (0), making for a total of `r Nclass["split"] + Nclass["0"]` OHS segments and `r Nclass["1"]` CDS segments (`r 100*round(Nclass["1"]/(Nclass["1"]+Nclass["0"]),3)`% CDS). 
The purpose of the classifier is thus to distinguish "pure" CDS from mixed or pure ODS.
A random 90% of the data (`r round(.9*(nrow(all_dat_cl)-Nclass["nap"]))` segments) were used to train the classifier, and the remaining 10% (`r round(.1*(nrow(all_dat_cl)-Nclass["nap"]))` segments) were used for evaluation.

```{r, eval=F, echo=F, include=F, results='hide'}
d_nonaps <- d %>% filter(cds_ohs!="nap")
dat <- add_features(d_nonaps, with_demo=F, prop_meaningful=F, per_child_norms=F) 

xgb <- readRDS("models/xgb_model.rds")
```



```{r retrain-on-all, echo=F, include=F, results='hide'}
d_nonaps <- d %>% filter(cds_ohs!="nap")
dat <- add_features(d_nonaps, with_demo=F, prop_meaningful=F, per_child_norms=F)

xgb_final <- readRDS("models/final_rawLENA_xgb_model.Rds")
# ToDo: roc and cmOOF are not in this data structure
# When trained on 90% of the segments using the normalized LENA features, the XGBoost classifier achieves an AUC of `r round(xgb_final$roc$auc,3)`, with an overall accuracy of `r round(xgb_final$cmOOF$overall['Accuracy'],3)` on the held-out data.

table(xgb_final$preds$cds_ohs, xgb_final$preds$CDSpred)
```


```{r, xgboost-raw-lena, echo=F, include=F, eval=F, caption="ROC and feature importance for the XGBoost classifier trained on normalized LENA features."}
dat$id = d_nonaps$id 

col_names = attr(xgb$train.data, ".Dimnames")[[2]]
imp = xgb.importance(col_names, xgb$model)

# plot AUC and importance
pdf("figs/rawLENA-xgboost-AUC-importance.pdf", width=8, height=4)
par(mfcol = c(1, 2), mai=c(0.7,0.0,0.2,0.1))
plot(xgb$roc)
text(x=.5, y=.1, paste0("AUC = ",round(auc(xgb$roc), 2))) 
#par(fig=c(0.5,1,0,1), new=TRUE)
xgb.plot.importance(imp, left_margin = 3, main="Feature Importance", cex=.8)
importance_matrix <- xgb.importance(col_names, model = xgb$model)
dev.off()
#print(importance_matrix)

ttpreds = xgb$ttdat
ttpreds$id = d_nonaps[rownames(xgb$ttdat),]$id
ttpreds$cos = d_nonaps[rownames(xgb$ttdat),]$cds_ohs # CDS / OHS / split

#write.csv(ttpreds, file="rawLENAxgb.csv")

cfm = table(ttpreds$cds_ohs, ttpreds$CDSpred)
#round(cfm / rowSums(cfm), 2)
#       0    1
#  0 0.73 0.27
#  1 0.25 0.75
```

```{r, caption="Histogram of classifier confidence by segment type."}
ttpreds = xgb_final$preds
ttpreds$cos = d_nonaps[rownames(xgb_final$preds),]$cds_ohs # CDS / OHS / split

d_all <- d_allfeat %>% filter(cds_ohs!="nap") %>%
  mutate(xgb_pred = xgb_final$preds$pred,
         CDSpred = xgb_final$preds$CDSpred)

#write.csv(ttpreds, file="rawLENAxgb.csv")

cfm = table(ttpreds$cds_ohs, ttpreds$pred)

high_conf_CDS <- subset(d_all, xgb_pred>0.7) # 2884 / 11057 = 26%
high_conf_ODS <- subset(d_all, xgb_pred<0.3) # 2196 / 11057 = 20%

table(high_conf_CDS$cds_ohs) / nrow(high_conf_CDS) # (2884 for .7)
table(high_conf_ODS$cds_ohs) / nrow(high_conf_ODS) # (2196 for .3)
# only 26% of the 'split' segments were given high-confidence ratings in the model (Pr(tCDS)<.3 or Pr(tCDS)>.7)

low_conf <- subset(d_all, xgb_pred>.4 & xgb_pred<.6) # 3136
#low_conf <- subset(d_all, xgb_pred>.3 & xgb_pred<.7) # 661/1012 splits in this range
table(low_conf$cds_ohs) # 1534 / 3136 = 49% are tCDS
# 370 splits (out of 1012) are in this range: 37%

ttpreds %>% mutate(`Segment Type` = case_when(
  cos==0 ~ "ODS",
  cos==1 ~ "tCDS",
  TRUE ~ cos
)) %>% 
  #filter(cos!="split") %>% # remove splits for BUCLD abstract
  ggplot(aes(x=pred, group=`Segment Type`, fill=`Segment Type`)) + 
   geom_histogram(alpha=.5, position = 'identity') + # facet_wrap(vars(cos)) + 
   theme_classic() + xlab("Classifier Pr(tCDS)") +
   geom_vline(xintercept=.5, linetype='dashed') + ylab("Number of Segments")
```


When using a more sophisticated classifier, there is a greater danger of overfitting to the dataset. 
Thus, in addition to training on 90% of the data and testing on the remaining 10%, we will also test the generality of the XGBoost classifier, we also conducted cross-validation checks by leaving out data from 10% of the children in each of ten folds (i.e., 10-fold cross-validation) and training the classifier on the remaining 90% of the children's data.

```{r, xgboost-raw-lena-children, echo=F, message=F, include=F, results='hide'}
dat <- add_features(d_nonaps, with_demo=F, prop_meaningful=F, per_child_norms=F)
dat$id = d_nonaps$id
xgb_ch = do_child_level_cv(dat, fname="raw", k=10) # AUC=.73, test=.66
#mean(xgb_ch$auc) # 10min: .72  5min: .76  with naps: .794
#mean(xgb_ch$test_acc) # 10min: .66  5min: .69 with naps: .71

#dat <- add_features(d_nonaps, with_demo=F, prop_meaningful=F, per_child_norms=F) 
#dat$id = d_nonaps$id
#xgb_ch_raw = do_child_level_cv(dat, fname="rawLENA", k=10) # acc: .66 auc: .73
#raw_ch_acc = mean(xgb_ch_raw$test_acc) # .66
#raw_ch_auc = mean(xgb_ch_raw$auc) # .73

dat <- add_features(d_nonaps, with_demo=F, prop_meaningful=T, per_child_norms=F) 
dat$id = d_nonaps$id
xgb_ch_meaningful = do_child_level_cv(dat, fname="meaningful", k=10) # acc: .67 auc: .73
meaning_ch_acc = mean(xgb_ch_meaningful$test_acc) # .67
meaning_ch_auc = mean(xgb_ch_meaningful$auc) # .73


# start to see overfitting
#dat <- add_features(d_nonaps, with_demo=F, prop_meaningful=F, per_child_norms=T) # bal acc: .74 (test ch CV)
#dat$id = d_nonaps$id
#xgb_ch3 = do_child_level_cv(dat, fname="per_child_norms", k=10) # acc: .63 auc: .69
#dat <- add_features(d_nonaps, with_demo=F, prop_meaningful=T, per_child_norms=T) # bal acc: .75 (test ch CV)
#dat$id = d_nonaps$id
#xgb_ch4 = do_child_level_cv(dat, fname="per_child_meaningful") # acc: .64 auc: .70
```

This children-left-out cross-validation had a mean test accuracy of `r round(meaning_ch_acc, 2)` (sd=`r round(sd(xgb_ch_meaningful$test_acc),2)`) and a mean AUC of `r round(meaning_ch_auc, 2)` (sd=`r round(sd(xgb_ch_meaningful$auc),2)`). 
Since these values are quite similar to the test accuracy and AUC from the segment-based cross-validation, we can rest assured that the XGboost model is not simply overfitting to each participant, and should therefore generalize to entirely new participants.
(GK: maybe move child-CV to the Appendix and mention in a footnote)

# Results

## Inter-rater reliability

Plan is to have 40 person-hours go into checking reliability. Should do some from each dataset, but oversampling from CONTX seems wise to double-check the 

## Validation of classifier
Label additional few hundred segments (from CONTX). 
(Can easily generate predictions from the classifier for all CONTX segments now!)

## Outcomes

```{r wf-outcomes, echo=F}
wf_outcomes <- read_csv("data/S-LENAdata-FINAL.csv") %>% 
  arrange(SubNum) %>%
  mutate(FreqOHShr = FreqOH_noMedia * 60) # AWC/min of OHS segments

wf_CDS_vs_vocab <- cor.test(wf_outcomes$FreqCDShr, wf_outcomes$vocab24) # .52 p<.01
wf_OHS_vs_vocab <- cor.test(wf_outcomes$FreqOHShr, wf_outcomes$vocab24) # .25
cor.test(wf_outcomes$FreqCDShr, wf_outcomes$FreqOHShr) # .17
with(wf_outcomes, plot(FreqCDShr, FreqOHShr))
```

One critical question is whether the classifier trained above works sufficiently well to replicate the results from manually-annotated studies.
To test this, we use the Weisleder & Fernald (2013) dataset of 29 Spanish-speaking children, who were tested at 19 months and again at 24 months of age.
In this dataset, children who heard more CDS at 19 months had significantly larger vocabularies at 24 months (`r apa_print(wf_CDS_vs_vocab)$full_result`).
Meanwhile, there was no significant relationship between the amount of overheard speech at 19 months and vocabulary size at 24 months (`r apa_print(wf_OHS_vs_vocab)$full_result`).
Now we use the model's predicted CDS/OHS segments instead of the manual annotations to see if we recover these same outcomes.

```{r outcomes-comparison}
wf_dat <- read.csv(here("data/WFdata_MediaSegmentstoExclude.csv"))
wf_media_exclusions <- wf_dat %>% filter(Media_exclude==1)
# ToDo: remove these 348 segments from the CDS / OHS counts, see if we match

subj_total_dur <- d_all %>%
  group_by(id, language, Dataset) %>%
  summarise(total_dur_hr = sum(dur_min) / 60) %>%
  arrange(id)

# grouped by human binary ratings of CDS / ODS
hum_per_hr <- d_all %>% 
  mutate(expCDS_AWC = xgb_pred * AWC,
         expODS_AWC = (1-xgb_pred) * AWC) %>%
  left_join(subj_total_dur) %>%
  group_by(id, language, Dataset, cds_ohs) %>% 
  summarise(total_dur_hr = total_dur_hr[1],
            expCDS_AWC = sum(expCDS_AWC) / total_dur_hr,
            expODS_AWC = sum(expODS_AWC) / total_dur_hr,
            AWC = sum(AWC) / total_dur_hr, 
            CTC = sum(CTC) / total_dur_hr, 
            CVC = sum(CVC) / total_dur_hr,
            noise = sum(noise_min) / total_dur_hr,
            silence = sum(silence_min) / total_dur_hr,
            tv = sum(tv_min) / total_dur_hr) %>%
  arrange(id) 

# grouped by binarized model prediction of CDS / ODS
mod_per_hr <- d_all %>% 
  left_join(subj_total_dur) %>%
  mutate(expCDS_AWC = xgb_pred * AWC,
         expODS_AWC = (1-xgb_pred) * AWC) %>%
  group_by(id, language, Dataset, CDSpred) %>% 
  summarise(total_dur_hr = total_dur_hr[1],
            expCDS_AWC = sum(expCDS_AWC) / total_dur_hr,
            expODS_AWC = sum(expODS_AWC) / total_dur_hr,
            AWC = sum(AWC) / total_dur_hr, 
            CTC = sum(CTC) / total_dur_hr, 
            CVC = sum(CVC) / total_dur_hr) %>%
  arrange(id) 

# high-confidence model predictions, grouped by binarized model CDS prediction
mod_hc_per_hr <- d_all %>%
  left_join(subj_total_dur) %>%
  filter(xgb_pred < .25 | xgb_pred > .75) %>% # keeps 3633 / 11057 (33%)
  mutate(expCDS_AWC = xgb_pred * AWC,
         expODS_AWC = (1-xgb_pred) * AWC) %>%
  group_by(id, CDSpred) %>% 
  summarise(total_dur_hr = total_dur_hr[1],
            expCDS_AWC = sum(expCDS_AWC) / total_dur_hr,
            expODS_AWC = sum(expODS_AWC) / total_dur_hr,
            AWC = sum(AWC) / total_dur_hr, 
            CTC = sum(CTC) / total_dur_hr, 
            CVC = sum(CVC) / total_dur_hr) %>%
  arrange(id) 

# total per-subject hourly AWC, etc, as well as model-expected AWC
# (ignores human ratings)
modexp_per_hr <- d_all %>%
  left_join(subj_total_dur) %>%
  mutate(expCDS_AWC = xgb_pred * AWC,
         expODS_AWC = (1-xgb_pred) * AWC) %>%
  group_by(id, Dataset, language, mom_ed, fat_ed) %>% 
  summarise(segments = n(),
            total_dur_hr = total_dur_hr[1],
            expCDS_AWC = sum(expCDS_AWC) / total_dur_hr,
            expODS_AWC = sum(expODS_AWC) / total_dur_hr,
            AWC = sum(AWC) / total_dur_hr, 
            CTC = sum(CTC) / total_dur_hr, 
            CVC = sum(CVC) / total_dur_hr) %>%
  arrange(id) 


# deal with splits
hum_per_hr_wide <- hum_per_hr %>%
  pivot_wider(id_cols=c("id","language","Dataset"), names_from=cds_ohs, values_from=AWC) %>%
  rename(OHS_AWChr = `0`,
         CDS_AWChr = `1`) %>%
  mutate(split = ifelse(is.na(split), 0, split)) %>%
  mutate(OHS_AWChr = OHS_AWChr + .5*split,
         CDS_AWChr = CDS_AWChr + .5*split) %>%
  dplyr::select(-split)


WFdata <- hum_per_hr_wide %>%
  filter(Dataset=="WF") %>%
  left_join(wf_outcomes, by=c("id"="SubNum"))


# do we get the same if we use classifier-predicted CDS (binarized)?
CDSpred <- wf_outcomes %>%
  left_join(mod_per_hr %>% filter(CDSpred==1), by=c("SubNum"="id"))
OHSpred <- wf_outcomes %>%
  left_join(mod_per_hr %>% filter(CDSpred==0), by=c("SubNum"="id"))
cor.test(CDSpred$vocab24, CDSpred$AWC) # r=.48 p<.01
cor.test(OHSpred$vocab24, OHSpred$AWC) # r=.32 p=.09

cor.test(OHSpred$AWC, CDSpred$AWC) # 0.16

p1 <- CDSpred %>% ggplot(aes(x=FreqCDShr, y=vocab24)) + geom_point() +
  geom_smooth(method='lm') + theme_classic() + 
  stat_cor(aes(label=..rr.label..), label.x=30, label.y=500) +
  xlab("Hand-coded tCDS") + ylab("Vocabulary at 24 months")
p2 <- CDSpred %>% ggplot(aes(x=AWC, y=vocab24)) + geom_point() +
  geom_smooth(method='lm') + theme_classic() + 
  stat_cor(aes(label=..rr.label..), label.x=30, label.y=500) +
  xlab("Model-predicted tCDS") + ylab("Vocabulary at 24 months")
ggarrange(p1, p2, nrow=1)
ggsave("figs/WF_outcomes_hand-coded_vs_modelCDS.pdf", width=6, height=3)


p1 <- OHSpred %>% ggplot(aes(x=FreqOH_noMedia*60, y=vocab24)) + geom_point() +
  geom_smooth(method='lm') + theme_classic() + 
  stat_cor(aes(label=..rr.label..), label.x=1, label.y=450) +
  xlab("Hand-coded ODS") + ylab("Vocabulary at 24 months")
p2 <- OHSpred %>% ggplot(aes(x=AWC, y=vocab24)) + geom_point() +
  geom_smooth(method='lm') + theme_classic() + 
  stat_cor(aes(label=..rr.label..), label.x=30, label.y=500) +
  xlab("Model-predicted ODS") + ylab("Vocabulary at 24 months")
ggarrange(p1, p2, nrow=1)
ggsave("figs/WF_outcomes_hand-coded_vs_modelOHS.pdf", width=6, height=3)

pred_vs_actual_CDS <- cor.test(CDSpred$AWC, CDSpred$FreqCDShr) # .78 - close!
pred_vs_actual_OHS <- cor.test(OHSpred$AWC, CDSpred$FreqOH_noMedia) # .88 - not too bad
modCDS_vs_vocab <- cor.test(CDSpred$AWC, wf_outcomes$vocab24) # orig: .53 / .48
modOHS_vs_vocab <- cor.test(OHSpred$AWC, wf_outcomes$vocab24) # .36 / .32 

# now use high-confidence binarized model predictions
CDSpred <- wf_outcomes %>%
  left_join(mod_hc_per_hr %>% filter(CDSpred==1), by=c("SubNum"="id"))
OHSpred <- wf_outcomes %>%
  left_join(mod_hc_per_hr %>% filter(CDSpred==0), by=c("SubNum"="id"))

modCDS_vs_vocab <- cor.test(CDSpred$AWC, wf_outcomes$vocab24) # now .44 p=.02
modOHS_vs_vocab <- cor.test(OHSpred$AWC, wf_outcomes$vocab24) # now .22 p=.25

# now use model expected CDS / ODS AWC
mod_expected <- wf_outcomes %>%
  left_join(modexp_per_hr, by=c("SubNum"="id"))

cor.test(mod_expected$expCDS_AWC, mod_expected$vocab24) # r=.56, t(27) = 3.53, p=.001
cor.test(mod_expected$expODS_AWC, mod_expected$vocab24) # r=.35 t(27) = 1.94, p=.06

cor.test(mod_expected$expCDS_AWC, mod_expected$FreqCDShr)

# how well can we predict vocab24 combining model-expected hourly CDS AWC and ODS AWC?
summary(lm(vocab24 ~ 0 + expCDS_AWC + expODS_AWC, data=mod_expected)) # r^2 = .82
summary(lm(vocab24 ~ 0 + expODS_AWC, data=mod_expected)) # r^2 = .71
summary(lm(vocab24 ~ 0 + expCDS_AWC, data=mod_expected)) # r^2 = .82
# r^2 = .82, coefficients: expCDS_AWC = 0.490, expODS_AWC = 0.005 (n.s.)
# (CDS alone coef: 0.31, r^2 = .71)

# compare to original vars:
summary(lm(vocab24 ~ 0 + FreqCDShr + FreqOH_noMedia, data=mod_expected))
# r^2 = .80

summary(lm(vocab24 ~ 0 + AWC + CTC + CVC, data=mod_expected)) # r^2=.808 AWC+*, CVC.
summary(lm(vocab24 ~ 0 + AWC + CTC, data=mod_expected)) # r^2=.78

summary(lm(vocab24 ~ 0 + AWC + CTC, data=mod_expected))

# look at means for WF-coded vs. classifier-predicted CDS/ODS

#per_hr %>% group_by(cds_ohs) %>%
#  summarise(AWC=mean(AWC), CTC=mean(CTC), CVC=mean(CVC))

#pred_outcomes %>% group_by(CDSpred) %>%
#  summarise(AWC=mean(AWC), CTC=mean(CTC), CVC=mean(CVC))
```


```{r}
require(GGally)
ggpairs(mod_expected, columns= c("CTC","AWC","FreqCDShr","FreqOH_noMedia","expCDS_AWC","expODS_AWC","vocab24"), 
        ggplot2::aes(alpha=.5)) + 
  theme_bw()
ggsave("figs/model_expectedCDS_vs_WForig.pdf", width=9, height=9)
```


```{r}
combo <- rbind(CDSpred, OHSpred)
ggpairs(combo, columns = c("AWC","FreqCDShr","FreqOH_noMedia","vocab24"), 
        ggplot2::aes(colour=as.factor(CDSpred), alpha=.4)) + 
  theme_classic()
```


Using the model's predictions, as in the original dataset, children who heard more CDS at 19 months had significantly larger vocabularies at 24 months (`r apa_print(modCDS_vs_vocab)$full_result`).
As with the original manual annotations, when using the model's predictions there was no significant relationship between the amount of overheard speech at 19 months and vocabulary size at 24 months (`r apa_print(modOHS_vs_vocab)$full_result`).
Moreover, the strength of these correlations are very similar to those found in the original analysis.


# General Discussion
Our study suggests that measures automatically-generated by LENA can facilitate identification of potentially meaningful sources of variation in children’s speech environments. We discuss our five main insights in turn.
First, we found differences in how commonly-used frequency measures of AWC, CTC, and CVC predicted the probability of a segment having target-child-directed speech. Our preliminary analyses indicated that segments with higher AWC relative to a family’s mean were more likely to be judged by humans as having primarily other-directed rather than target-child-directed speech. Frequency measures of CTC and CVC resulted in the opposite prediction, where segments with higher values relative to a family’s mean were more likely to be judged as having predominantly target-child-directed speech. These findings suggest that periods of speech directed to a target child are defined by relatively lower rates of adult words and relatively higher rates of conversational turns and child vocalizations. This is consistent with the finding that adults often use a slower speech-rate when talking with children and that target-child-directed speech is more likely to elicit vocalizations from the child than other-directed speech. This finding also suggests that one reason some studies have found LENA’s CTC measure to be a better predictor of child language outcomes than AWC (Gilkerson et al., 2018; Romeo et al., 2018) may be that high CTC is a better indicator of periods with target-child-directed speech than AWC.

Second, a much more complex picture arose when including both LENA frequency and duration measures in cluster analyses. While some distinct features characterized different audio environments, there was also a high degree of overlap across clusters. For example, as expected, clusters with more sleep segments were characterized by the lowest rates of AWC, CTC, and CVC. However, one sleep cluster was characterized by more silence, while the other was characterized by more noise. This aligns with anecdotal reports by human coders that periods of sleep sometimes involved what appeared to be fans or sound machines, sounds which were likely categorized as “noise” by LENA. Baby snores, which also sometimes occurred during periods of sleep, could also have been categorized as “noise” by LENA. In contrast, those clusters that were likely to be tCDS were characterized by the highest averages of CTC and CVC, but were more mixed with regards to AWC. Of clusters likely to be ODS, one cluster consisted of the highest average AWC, while the others had lower CTC and CVC rates, or longer durations of distant speech and TV. Thus, we observed multiple ways in which features were combined in clusters of predominantly sleep, tCDS, and ODS.  Moreover, in no cases was sleep, tCDS, or ODS associated with only one cluster or configuration of features. Future work might fruitfully examine in more detail potential differences between segments in different cluster types. For example, are segments in some clusters associated with different types of language interactions and/or activities than other segments?

Third, we found a high degree of success in training a classifier to identify periods of sleep. Using a simple decision tree classifier, AUC approached .90 for both the full dataset and the held-out test segments. Consistent with the multifaceted nature of clusters defined by more sleep, the classification was not simply due to periods of silence. The decision tree first considered the duration of ‘meaningful’ speech, then the duration of silence, before finally considering the number of children’s own vocalizations. This suggests that periods in which the target child is asleep  vs. awake could be reliably identified from characteristics of the audio environment and shows advantages of considering multiple features of those environments. 

Fourth, we found moderate success in training a classifier to distinguish periods of tCDS versus ODS. To accommodate the overlapping clusters of tCDS and ODS, we used a more sophisticated machine-learning model, XGBoost, where the model could sequentially add a cascade of decision trees and weight misclassifications by earlier decision trees. We found moderate sensitivity and specificity on the full dataset and a slightly weaker AUC on the held-out test segments. The feature importance list illustrated the average gain in our prediction of tCDS versus ODS, highlighting many features (meaningful speech, AWC, CTC, and silence) that also emerged in our cluster analysis. Reliability between two trained human raters suggests that even when individuals undergo training and interpret all available information in the auditory environment, there is variability across samples and there may be a ceiling of ‘good enough’ reliability. The moderate success of the classifier in terms of sensitivity and specificity, as well as performance seen in the confusion matrices were similar to that of two human raters, which suggests that the success of the classifier may be a reasonable goal given the complexities of the speech environment. The superior performance of the classifier relative to analyses with individual predictors suggests that human classifications of target-child-directed and other-directed speech rely on nuanced distinctions that take into account combinations of features in the audio environment (e.g., low silence with high CTC and moderate AWC), as well as potentially features of the environment not captured by these measures (e.g., semantic content).

Finally, we demonstrated that we could use model-derived predictions of tCDS and ODS to replicate associations between caregiver speech at 19 months and children’s vocabularies at 24 months that were observed in previously published work (Weisleder & Fernald, 2013). The model-predicted classifications revealed, as previously observed with hand-coding, that variability in adults’ directed speech to target children was positively and significantly correlated with children’s later vocabularies, whereas this link was not statistically significant when adult speech was directed to others. 

# Suggested uses of the classifier
We constructed a web app (https://kachergis.shinyapps.io/classify_cds_ods/) deploying the final XGboost model along with the final sleep decision-tree classifier, so that other researchers with daylong LENA recordings can easily use it on their datasets. However, we encourage researchers with populations dissimilar to those studied here to conduct their own checks for the generalizability of the predictions to new samples (see Limitations below). 

Taken together, our findings suggest that applying classifiers to LENA data may facilitate data cleaning, coding, and analysis, to better specify the amount of speech directed to target children and speech directed to others. First, the sleep classifier can automate one laborious step of ‘cleaning’ daylong LENA recordings with a reasonably high degree of reliability. Second, the tCDS/ODS classifier could also be used to reduce the significant hours of manual labor required for coding periods of target-child-directed or other-directed speech. We have found that the classifier’s per-segment probability of tCDS matches well with the uncertainty of human coders (e.g., the 50/50 “split” segments were classified as ~50% probability of being tCDS). 

We suggest three potential workflows, illustrated in Figure 8. Option 1 would consist of first running the sleep classifier to exclude periods when the child is sleeping and thus less likely to learn from the available speech, then running the tCDS/ODS classifier to identify binary judgements of segments considered as tCDS or ODS. Option 2 could be to follow the same steps, but rather than use a binary tCDS or ODS judgment, use the probabilities of tCDS or ODS to identify ‘high confidence’ tCDS (or ODS) segments versus ‘low confidence’ tCDS (or ODS) segments; ‘low confidence’ segments could then be listened to and judged by human coders. Whichever values are chosen, it is recommended to choose values that are symmetric (e.g., Pr(tCDS) < 0.3 (i.e., ODS) and Pr(tCDS) > 0.7 (i.e., tCDS)), to limit the introduction of bias. Alternatively, Option 3 would be to use the classifier probabilities to estimate the number of AWC tokens of tCDS and ODS in each segment by computing expected value (see Appendix D for more explanation). For example, a segment with an AWC of 200 and a .7 probability of being tCDS would result in 140 adult words counted as tCDS and 60 words counted as ODS. See Appendix D for an application of this method to the Weisleder & Fernald (2013) data, which yielded similar associations with outcomes. It is important to note that higher tCDS probabilities may reflect more of a certain type of verbal interaction (e.g., one-on-one interactions in a quiet indoor setting) than other types of caregiver-child interactions (e.g., playing outside where speakers may be further away from each other). Therefore, how probabilities are used should be considered with caution and transparently documented to better understand their utility and significance. 

[insert Figure 8 here]

# Limitations
While we included over 1,000 hours of data from 153 English- and Spanish-speaking families from varied socioeconomic backgrounds, our sample represents a small subset of the variability that exists within English- and Spanish-speaking families in the U.S. Our sample also represents a tiny subset of the linguistic (e.g., multiple languages, signed vs. spoken language, multilingualism) cultural, and ecological variability in child-rearing environments around the world. For example, there is wide variability in infant sleep routines seen across families and countries (Mindell et al., 2010), which are not represented in this current work. Thus, it is very possible that the acoustic features that characterize periods of sleep (and tCDS and ODS) in our recordings will not generalize to recordings collected in very different contexts. In addition, all of the families in our studies lived in urban settings, and it is likely that the acoustic features that characterize periods of sleep, tCDS and ODS would differ for families in different settings (e.g., subsistence farming communities). Further validation studies are critical to understand whether our classifiers can generalize to new languages and communities (Cristia et al., 2021). Additionally, while our classifier is open-source, LENA software is not; thus, the ability to use this classifier on researchers’ own data requires a substantial cost to purchase the LENA recorders and software. Future work should compare whether our classifiers can be used with open-source speech algorithms (e.g., ALICE; Räsänen et al., 2021) to achieve similar performance in sleep and tCDS/ODS classifiers.

# Conclusion
The findings here present exciting opportunities for advancement in understanding how children learn from the available speech in their environment. We were able to train and validate two automated classifiers using LENA-based measures to identify periods of sleep and to distinguish between periods of tCDS versus ODS. This work has the potential to significantly reduce the time-consuming process of identifying periods of directed speech to target children from the rich and naturalistic information collected with daylong recordings, thus facilitating research that would illuminate questions about the relations between target-child-directed and other-directed speech on child outcomes and/or about the features of child-directed speech across linguistically- and culturally-diverse communities. We hope that by improving our methods to explore shared and different features of target-child- and other-directed speech, we can better understand how different children across diverse communities acquire and develop their language skills.

### Data, code and materials availability statement. 
All data and analysis code for the app are available here: [https://github.com/kachergis/classify_cds_ods](https://github.com/kachergis/classify_cds_ods). All analysis code for the current manuscript are available here: [https://github.com/kachergis/tCDS_nap_classifier_paper](https://github.com/kachergis/tCDS_nap_classifier_paper)


### Ethics statement. 
Ethics approval was obtained from a university institutional review board.


### Authorship and contributorship. 
All authors contributed to conceptualization of the present study. J.B. (logistic regressions and reliability) and G.K. (classifiers) contributed to analyses. J.B., A.W., and V.M. supervised and analyzed original coding of LENA data across the five samples. J.B. and G.K. contributed to the original draft preparation. All authors contributed to writing, reviewing, and editing of the final manuscript.


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
